[
  {
    "theme": "Code RL environments",
    "signals": [
      "Terminal capability scaling: command-line interaction as a first-class RL environment.",
      "Hardware-aware optimization: CUDA kernel generation framed as search with intrinsic world models.",
      "Formal verifiable research agents: proof systems as reward environments.",
      "Reward foundries: automatic verifiers (tests/simulators) to reduce per-step human labeling cost."
    ]
  },
  {
    "theme": "Human data and feedback",
    "signals": [
      "Shift from classic RLHF preference labeling to HITL orchestration and long-horizon trajectory collection.",
      "Trajectory-splitting SFT as a cold-start bridge for sparse long-horizon human data.",
      "Progressive RL with longer timeout stages to preserve learning signal over thousands of steps.",
      "Synthetic trajectory distillation from frontier models to bootstrap scarce domains."
    ]
  },
  {
    "theme": "Ecosystem debate",
    "signals": [
      "RLVR vs RLHF remains active: verifiability improves robustness, but open-ended task verification remains difficult.",
      "Rubric-based RL is resurfacing with stronger historical continuity to safety-alignment methods.",
      "Infrastructure moat uncertainty: tasks/rubrics/verifiers vs full harness + TVR stacks."
    ]
  }
]
