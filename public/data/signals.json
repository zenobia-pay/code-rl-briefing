[
  {
    "section": "Key Trends in Code RL Environments",
    "points": [
      "Shift from static code generation to interactive agent loops (execute commands, observe errors, iterate).",
      "Terminal Capability Scaling: command-line interfaces treated as structured RL environments for filesystem and system-level tasks.",
      "GPU Kernel Optimization: K-Search frames CUDA kernel generation as search using a co-evolving intrinsic world model to reduce expensive hardware rollouts.",
      "Mathematical Research Agents: formal proof systems provide verifiable rewards for autonomous research agents."
    ]
  },
  {
    "section": "Advancements in Human Data & Feedback",
    "points": [
      "Shift from pure preference labeling (RLHF) to HITL orchestration plus long-horizon trajectory collection.",
      "KLong proposes trajectory-splitting SFT to cold-start long trajectories while preserving early context and progressively truncating later context with overlap.",
      "Progressive RL extends timeout horizons stage-by-stage to preserve learning signal over long trajectories.",
      "Research-Factory distills synthetic long-horizon trajectories to address scarcity of costly human trajectory data.",
      "Verifiable reward foundries reduce per-step human labeling by using tests/simulators/formal checks."
    ]
  }
]
